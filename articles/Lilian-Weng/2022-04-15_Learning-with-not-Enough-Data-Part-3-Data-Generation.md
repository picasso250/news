[Lil'Log](https://lilianweng.github.io/) | [Posts](https://lilianweng.github.io/) [Archive](https://lilianweng.github.io/archives) [Search](https://lilianweng.github.io/search/) [Tags](https://lilianweng.github.io/tags/) [FAQ](https://lilianweng.github.io/faq)
# Learning with not Enough Data Part 3: Data Generation
Date: April 15, 2022 | Estimated Reading Time: 28 min | Author: Lilian Weng Table of Contents [Data Augmentation](#data-augmentation) [Image Augmentation](#image-augmentation) [Basic Image Processing Operations](#basic-image-processing-operations) [Task-Specific Augmentation Strategies](#task-specific-augmentation-strategies) [Image Mixture](#image-mixture) [Text Augmentation](#text-augmentation) [Lexical Edits](#lexical-edits) [Back-translation](#back-translation) [Mix-up](#mix-up) [Audio Augmentation](#audio-augmentation) [Architectural Augmentation](#architectural-augmentation) [Data Synthesis](#data-synthesis) [Language Model as Noisy Annotator](#language-model-as-noisy-annotator) [Language Model as Data Generator](#language-model-as-data-generator) [How to Quantify Generated Data Quality?](#how-to-quantify-generated-data-quality) [Training with Noisy Data](#training-with-noisy-data) [Regularization and Robust Architecture](#regularization-and-robust-architecture) [Robust Learning Objective](#robust-learning-objective) [Label Correction](#label-correction) [Sample Reweighting and Selection](#sample-reweighting-and-selection) [Citation](#citation) [Reference](#reference)
Here comes the Part 3 on learning with not enough data (Previous: [Part 1](https://lilianweng.github.io/posts/2021-12-05-semi-supervised/) and [Part 2](https://lilianweng.github.io/posts/2022-02-20-active-learning/) ). Let‚Äôs consider two approaches for generating synthetic data for training.
Augmented data . Given a set of existing training samples, we can apply a variety of augmentation, distortion and transformation to derive new data points without losing the key attributes. We have covered a bunch of augmentation methods on text and images in a [previous post](https://lilianweng.github.io/posts/2021-05-31-contrastive/) on contrastive learning. For the sake of post completeness, I duplicate the section on data augmentation here with some edits. New data . Given few or even no data points, we can rely on powerful pretrained models to generate a number of new data points. This is especially true in recent years given the fast progress in large pretrained [language models (LM)](https://lilianweng.github.io/posts/2019-01-31-lm/) . Few shot prompting is shown to be effective for LM to learn within context without extra training.
# Data Augmentation [#](#data-augmentation)
The goal of data augmentation is to modify the input format (e.g. text wording, visual appearance) while the semantic meaning stays unchanged.
## Image Augmentation [#](#image-augmentation)
### Basic Image Processing Operations [#](#basic-image-processing-operations)
There are several ways to modify an image while retaining its semantic information. We can use any one of the following augmentation or a composition of multiple operations.
Random cropping and then resize back to the original size. Random color distortions Random Gaussian blur Random color jittering Random horizontal flip Random grayscale conversion And many more. Check [PIL.ImageOps](https://pillow.readthedocs.io/en/stable/reference/ImageOps.html) for inspiration.
### Task-Specific Augmentation Strategies [#](#task-specific-augmentation-strategies)
If the downstream task is known, it is possible to learn the optimal augmentation strategies (i.e. what processing operations to use and how to combine them in sequence) to maximize the downstream task performance.
[AutoAugment](https://lilianweng.github.io/posts/2019-05-05-domain-randomization/#AutoAugment) ( [Cubuk, et al. 2018](https://arxiv.org/abs/1805.09501) ) is inspired by [neural architecture search](https://lilianweng.github.io/posts/2020-08-06-nas/) , AutoAugment frames the problem of learning best data augmentation operations (i.e. shearing, rotation, invert, etc.) for image classification as an RL problem and looks for the combination that leads to the highest accuracy on the evaluation set. AutoAugment can be executed in adversarial fashion ( [Zhang, et al 2019](https://arxiv.org/abs/1912.11188) ). RandAugment ( [Cubuk et al., 2019](https://arxiv.org/abs/1909.13719) ) greatly reduces the search space of AutoAugment by controlling the magnitudes of different transformation operations with a single magnitude parameter. Population based augmentation (PBA; [Ho et al., 2019](https://arxiv.org/abs/1905.05393) ) combines PBT (‚Äúpopulation based training‚Äù; [Jaderberg et al, 2017](https://arxiv.org/abs/1711.09846) ) with AutoAugment, using the evolutionary algorithm to train a population of children models in parallel to evolve the best augmentation strategies. Unsupervised Data Augmentation (UDA; [Xie et al., 2019](https://arxiv.org/abs/1904.12848) ), among a set of possible augmentation strategies, selects a subset to minimize the KL divergence between the predicted distribution over an unlabelled example and its unlabelled augmented version.
### Image Mixture [#](#image-mixture)
Image mixture methods can construct new training examples from existing data points.
Mixup ( [Zhang et al., 2018](https://arxiv.org/abs/1710.09412) ) runs global-level mixture by creating a weighted pixel-wise combination of two existing images I 1 and I 2 : I mixup ‚Üê Œ± I 1 + ( 1 ‚àí Œ± ) I 2 and Œ± ‚àà [ 0 , 1 ] . Cutmix ( [Yun et al., 2019](https://arxiv.org/abs/1905.04899) ) does region-level mixture by generating a new example by combining a local region of one image with the rest of the other image. I cutmix ‚Üê M b ‚äô I 1 + ( 1 ‚àí M b ) ‚äô I 2 , where M b ‚àà { 0 , 1 } I is a binary mask and ‚äô is element-wise multiplication. It is equivalent to filling the cutout ( [DeVries & Taylor 2017](https://arxiv.org/abs/1708.04552) ) region with the same region from another image. Given a query q , MoCHi (‚Äúmixing of contrastive hard negatives‚Äù; [Kalantidis et al. 2020](https://arxiv.org/abs/2010.01028) ) maintains a queue of K negative features Q = n 1 , ‚Ä¶ , n K and sorts these negative features by similarity to the query, q ‚ä§ n , in descending order. The first N items in the queue are considered as the hardest negatives, Q N . Then synthetic hard examples can be generated by h = h ~ / | h ~ | 2 where h ~ = Œ± n i + ( 1 ‚àí Œ± ) n j and Œ± ‚àà ( 0 , 1 ) . Even harder examples can be created by mixing with the query feature, h ‚Ä≤ = h ‚Ä≤ ~ / | h ‚Ä≤ ~ | 2 where h ‚Ä≤ ~ = Œ≤ q + ( 1 ‚àí Œ≤ ) n j and Œ≤ ‚àà ( 0 , 0.5 ) .
## Text Augmentation [#](#text-augmentation)
### Lexical Edits [#](#lexical-edits)
Easy Data Augmentation (EDA; [Wei & Zou 2019](https://arxiv.org/abs/1901.11196) ) defines a set of simple but powerful operations for text augmentation. Given a sentence, EDA randomly chooses and applies one of four simple operations:
Synonym replacement (SR): Replace n random non-stop words with their synonyms. Random insertion (RI): Place a random synonym of a randomly selected non-stop word in the sentence at a random position. Random swap (RS): Randomly swap two words and repeat n times. Random deletion (RD): Randomly delete each word in the sentence with probability p .
where p = Œ± and n = Œ± √ó sentence_length , with the intuition that longer sentences can absorb more noise while maintaining the original label. The hyperparameter Œ± roughly indicates the percent of words in one sentence that may be changed by one augmentation.
EDA is shown to improve the classification accuracy on several classification benchmark datasets compared to baseline without EDA. The performance lift is more significant on a smaller training set. All the four operations in EDA help improve the classification accuracy, but get to optimal at different Œ± ‚Äôs.
EDA leads to performance improvement on several classification benchmarks. (Image source: [Wei & Zou 2019](https://arxiv.org/abs/1901.11196) )
Contextual Augmentation ( [Kobayashi, 2018](https://arxiv.org/abs/1805.06201) ) replaces word w i at position i by sampling from a probability distribution learned by a bidirectional LM such as BERT, p ( . ‚à£ S ‚àñ w i ) . In this way, the words are substituted by synonyms, or similar words suitable for the context. To guarantee such operations do not alter the labels, the LM is fit to be label-conditioned bidirectional LM. Conditional BERT (CBERT; [Xing Wu et al. 2018](https://arxiv.org/abs/1812.06705) ) extends BERT to predict masked tokens conditioned on the class label and can be used for contextual augmentation prediction.
### Back-translation [#](#back-translation)
Back-translation produces augmented data by translating text samples to another language and then translating them back. The translation happens in two ways and both directions should have decent enough performance to avoid significant loss of semantic meaning.
### Mix-up [#](#mix-up)
It is also possible to apply [Mixup](#image-mixture) to text ( [Guo et al. 2019](https://arxiv.org/abs/1905.08941) ) but on the embedding space to obtain some performance gain. The proposed method relies on a specially designed model architecture to operate the prediction on the word or sentence embedding. Adding adversarial noise in the embedding space as a way of data augmentation is shown to improve the generalization of model training ( [Zhu et al. 2019](https://arxiv.org/abs/1909.11764) ).
## Audio Augmentation [#](#audio-augmentation)
Here is a list of several commonly used audio data augmentation methods, operated on raw audio or spectrograms, summarized by [Wang & van den Oord (2021)](https://arxiv.org/abs/2103.06508) .
Audio mixup. Given two audio clips x 1 and x 2 , the mixed-up version x ^ = Œ± x 1 + ( 1 ‚àí Œ± ) x 2 should be associated with the label of the more dominant input. The audio mixup augments the data with more realistic noise.
Time masking. A small consecutive chunk of the audio can be masked without losing semantic information.
Frequency masking. A small amount of frequency components on the spectrogram can be dropped off and it should not change the associated label.
Frequency shift. The spectrogram can be shifted by an integer between [ ‚àí F , F ] , where F is the maximum shift size. It is a cheap augmentation to change the pitch of the audio.
## Architectural Augmentation [#](#architectural-augmentation)
Models with dropout layers can create augmented samples by applying different dropout masks on the same input sample. For example, in the contrastive learning model [SimCSE](https://lilianweng.github.io/posts/2021-05-31-contrastive/#simcse) ( [Guo et al. 2021](https://arxiv.org/abs/2104.08821) ), a sample is simply fed into the encoder twice with different dropout masks and these two versions are the positive pair where the other in-batch samples are considered as negative pairs.
Dropout augments data by adding noise onto the internal representation of the model. It can be applied in a more structured way, such as in cutoff ( [Shen et al. (2020)](https://arxiv.org/abs/2009.13818) ), where random chunks of the token embedding matrix are removed.
# Data Synthesis [#](#data-synthesis)
Given that generating high-quality, photorealistic images is a lot more difficult than generating human-like natural language text and recent success with large pretrained language models, this section only focuses on text generation. To read more on how to synthesize realistic images, check posts on [GAN](https://lilianweng.github.io/posts/2017-08-20-gan/) , [VAE](https://lilianweng.github.io/posts/2018-08-12-vae/) , [flow](https://lilianweng.github.io/posts/2018-10-13-flow-models/) and [diffusion](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) models.
## Language Model as Noisy Annotator [#](#language-model-as-noisy-annotator)
[Wang et al. (2021)](https://arxiv.org/abs/2108.13487) explored ways to leverage GPT-3 as a weak annotator via few-shot prompting, achieving 10x cheaper than human labeling. The paper argues that by using data labeled by GPT-3, it essentially performs [self-training](https://lilianweng.github.io/posts/2021-12-05-semi-supervised/#self-training) : The predictions on unlabeled samples apply entropy regularization on the model to avoid high class overlaps so as to help improve the model performance.
Illustration of how to use GPT-3 to generate more training data with the human-in-the-loop active learning pipeline to improve the data quality. (Image source: [Wang et al. 2021](https://arxiv.org/abs/2108.13487) )
GPT-3-labeled samples selected by [active learning](https://lilianweng.github.io/posts/2022-02-20-active-learning/) with highest uncertainty are sent to human labelers to be re-annotated. The few-shot prompt contains a small number of human labeled examples and thus the labeling cost is restricted. Synthetic samples are ranked by predicted logits of label y and those with the lowest scores go through relabeling.
GPT-3 labeling achieves better results in the low-cost regime, but has a gap with human labeling when enough money is spent on data collection. This implies the following inequation, although to what extent ‚Äúa lot‚Äù or ‚Äúnoisy‚Äù means depends on the task details.
A lot of high-quality data > A lot of noisy data > A little high quality data .
GPT-3 labeling technique improves the classification performance in the low-cost regime. (Image source: [Wang et al. 2021](https://arxiv.org/abs/2108.13487) )
## Language Model as Data Generator [#](#language-model-as-data-generator)
If enough training dataset for text classification tasks are available, we can fine-tune language models to synthesize more training samples conditioned on labels ( [Anaby-Tavor et al. 2019](https://arxiv.org/abs/1911.03118) , [Kumar et al. 2021](https://arxiv.org/abs/2003.02245) ).
Language-model-based data augmentation ( LAMBADA ; [Anaby-Tavor et al. 2019](https://arxiv.org/abs/1911.03118) ) takes such an idea, where the process involves fine-tuning both a classifier and a sample generation model.
Train a baseline classifier using the existing training dataset: h = A ( D train ) . Independently of step 1, a LM M is fine-tuned on D train to obtain M tuned . Synthesize a labeled dataset D ‚àó by generating the continuation of the sequence y[SEP] until EOS using M tuned . Filter synthesized dataset by, (1) Verifying that the predicted label is correct h ( x ) = y ; (2) Selecting the top ranked samples when they are ranked by the classifier probability. D syn ‚äÇ D ‚àó . They generate 10x more samples needed for augmentation and only the top 10% synthesized samples with highest confidence scores remain.
The final classifier is trained on D syn ‚à™ D train . The process can be repeated multiple times, but it is unclear whether the benefit would quickly diminish or the repetitive process would bring in self-bias.
Accuracy of LAMBADA vs. other generative approaches over all datasets and classifiers. (Image source: [Anaby-Tavor et al. 2019](https://arxiv.org/abs/1911.03118) )
To simplify LAMBADA, we can actually remove the dependency of a fine-tuned generation model and an existing training dataset of a decent size ( [Step 2](#step2) above). Unsupervised data generation ( UDG ; [Wang et al. 2021](https://arxiv.org/abs/2109.09193) ) relies on few-shot prompting on a large pretrained language model to generate high-quality synthetic data for training. Opposite to the above approach where LM is asked to predict y given x , UDG instead synthetizes the inputs x given labels y . Then a task-specific model is trained on this synthetic dataset.
[Schick & Schutze (2021)](https://arxiv.org/abs/2104.07540) proposed a similar idea but on the NLI task instead of classification, asking PLM to write sentence pairs that are similar or different while the model is prompted with task-specific instructions.
Illustration of the unsupervised data generation (UDG) framework. (Image source: [Wang et al., 2021](https://arxiv.org/abs/2109.09193) )
The few-shot prompts of UDG contain a small number of unlabeled examples, as well as a task-specific natural language description of the desired label. Because some generated examples are noisy, they implemented noisy label annealing ( NLA ) techniques to filter potentially misaligned samples out during the training processes. NLA gradually removes noisy training signals in time during training when the model starts to disagree with its pseudo label with high confidence. At each training step t , a given example ( x i , y ^ i ) is considered noisy and should be removed if:
The model predicted probability is higher than a threshold p ( y ¬Ø i | x i ) > Œº t where y ¬Ø i = arg ‚Å° max y p ( y | x i ) ; And the predicted label is different from the synthetic label, y ¬Ø i ‚â† y ^ i .
Note that the threshold Œº t is time-dependent, initialized as 0.9 and then gradually annealed to 1 / num_of_classes in time.
As shown in their experiments, the improvement of UDG over few-shot inference is quit significant, where NLA brings in some extra boost. The results are even comparable with supervised fine-tuning on several cases.
Comparison of accuracy of UDG and other methods on different classification datasets. (Image source: [Wang et al., 2021](https://arxiv.org/abs/2109.09193) )
[Han et al (2021)](https://arxiv.org/abs/2110.05448) achieved SOTA results on translation tasks using few-shot data generation, distillation and back-translation. The proposed method contains the following steps, assuming no access to paired translation data:
Zero-shot Generation. First use the zero-shot translation ability of a pre-trained LM to generate translations for a small set of unlabeled sentences. Few-shot Generation. Then amplify these zero-shot translations by using them as few-shot demonstrations to gather an even larger synthetic dataset. Distillation. Fine-tune the model on this dataset. The translation task is formulated as a language modeling task [L1] <seq1> [[TRANSLATE]] [L2] <seq2>. given a pair of two sequences <seq1, seq2> in two different languages. At test-time, the LM is prompted with [L1] <seq> [[TRANSLATE]] [L2] and a candidate translation <sampledSeq> is parsed from the sampled completion. Back-translation. Continue fine-tuning on the back-translation dataset where the order of samples is reversed, <sampledSeq, seq> . Step 1-4 can be repeated. Algorithm of using distillation and back-translation to train a language model on translation tasks. (Image source: [Han et al. 2021](https://arxiv.org/abs/2110.05448) )
The success of the above method depends on a good pretrained LM to kick off the initial translation dataset. Iterative few-shot generation and distillation with back-translation is an effective way to extract and refine the translation capability out of a pretrained LM and further to distill that into a new model.
Comparison of BLEU scores of the translation models of different training runs using: only distillation, back-translation, both and with more monolingual training data. (Image source: [Han et al. 2021](https://arxiv.org/abs/2110.05448) )
# How to Quantify Generated Data Quality? [#](#how-to-quantify-generated-data-quality)
Given all the generated data, either by data augmentation or data synthesis, how can we quantify data quality in terms of how they improve model generalization? [Gontijo-Lopes et al. (2020)](https://arxiv.org/abs/2002.08973) introduced two dimensions to track, affinity and diversity.
Affinity is a model-sensitive metric for distribution shift , quantifying how much an augmentation shifts the training data distribution from what a model learned. Definition: The performance difference between the model tested on clean data vs augmented data, while the model is trained on clean data. As a comparison, KL can also measure distribution shift but does not consider the model performance. Diversity is a measure of augmentation complexity , measuring the complexity of the augmented data with respect to the model and learning procedure. Definition: The final training loss of a model trained with a given augmentation. Another potential diversity measure is the entropy of the transformed data. A third potential diversity measure is the training time needed for a model to reach a given training accuracy threshold. All three metrics above are correlated.
The final model performance is dependent on both metrics to be high enough.
(a) Left: A scatter plot of affinity vs diversity metric, where each point represents a different augmentation method and its color indicates the final test accuracy. (b) Right: The conceptual illustration of the relationship between clean and augmented data in different regions of affinity and diversity metrics. (Image source: [Gontijo-Lopes et al. 2020](https://arxiv.org/abs/2002.08973) )
There are many quantitative metrics on relevancy and diversity, in different formations depending on whether a reference is available, such as perplexity, BLEU for text and inception score for images. I‚Äôm skipping the list of concrete quantitative metrics on quality here, given it could be very long.
# Training with Noisy Data [#](#training-with-noisy-data)
It is convenient to collect a large amount of noisy data via model generation or data augmentation, but it is hard to guarantee that augmented and generated data can be 100% accurate. Knowing that deep neural networks can easily overfit noisy labels and ‚Äúmemotize‚Äù corrupted labels, we can apply the techniques for training on noisy labels ( noise-robust training ) when using generated data to stabilize and optimize the performance. Please check this [survey paper (Song et al. 2021)](https://arxiv.org/abs/2007.08199) on learning from noisy labels for a more thorough coverage of related work.
## Regularization and Robust Architecture [#](#regularization-and-robust-architecture)
Generally speaking, mechanisms designed for avoiding overfitting should help improve training robustness when working with moderately noisy data, such as weight decay, dropout, batch normalization. In fact, good data augmentation (i.e. only non-essential attributes are modified) can be considered as a way of regularization as well.
A different approach is to enhance the network with a dedicated noisy adaptation layer to approximate the unknown projection of label corruption ( [Sukhbaatar et al. 2015](https://arxiv.org/abs/1406.2080) , [Goldberger & Ben-Reuven, 2017](https://openreview.net/forum?id=H12GRgcxg) ).
[Sukhbaatar et al. (2015)](https://arxiv.org/abs/1406.2080) introduced an extra linear layer Q into the network architecture to adapt the predictions to match the noisy label distribution. The noise matrix Q is initially fixed to the identity function while only the base model parameters is updated. After some time, Q starts to be updated and expected to capture the noise in the data. The noise matrix is trained with regularization to encourage it to match the noise distribution while keeping the base model prediction accurate for true labels.
(a) Left: A noise matrix Q is added between softmax and the final output for the loss. (b) Right: The noise matrix Q is fixed at the identity function initially and only gets updated with regularization after some training. (Image source: [Sukhbaatar et al. 2015](https://arxiv.org/abs/1406.2080) )
However, it is hard to guarantee such a noise matrix layer would only capture the noise transition distribution and it is actually non-trivial to learn. [Goldberger & Ben-Reuven (2017)](https://openreview.net/forum?id=H12GRgcxg) ) proposed to add an additional softmax layer end-to-end with the base model and apply the [EM algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) by treating the correct labels as latent random variable and the noise processes as a communication channel with unknown parameters.
## Robust Learning Objective [#](#robust-learning-objective)
Besides the most commonly used cross entropy loss, some other choices of learning objectives are shown to be more robust to noisy labels.
For example, MAE (mean absolute error) is more robust to noisy labels than CCE (categorical cross entropy), as it treats every sample equally ( [Ghosh et al. 2017](https://arxiv.org/abs/1712.09482) ). Lack of different weighting among training samples of MAE lead to significantly longer training time. Motivated by the tradeoff between MAE and CCE, [Zhang & Sabuncu (2018)](https://arxiv.org/abs/1805.07836) proposed generalized cross entropy ( GCE ), a generalization of CCE loss to be robust to noisy data.
To exploit the benefits of both the noise-robustness provided by MAE and the implicit weighting scheme of CCE, GCE adopts the the negative Box-Cox transformation as a loss function:
L q ( f ( x i , y i = j ) ) = 1 ‚àí f ( j ) ( x i ) q q
where f ( j ) denotes the j -th element of f ( . ) and q ‚àà ( 0 , 1 ] . L q is equivalent to CCE when q ‚Üí 0 and becomes MAE when q = 1 . Empirical experiments show that there exists a threshold of q with which overfitting never emerges and the noisier the data the higher such a threshold should be.
Given true and predicted labels, y i , y ^ i ‚àà { 0 , 1 } and let u i = y i ‚ãÖ y ^ i , the zero-one loss , ùüô L 01 ( u ) = ‚àë i = 1 n 1 [ u i < 0 ] , is another learning subjective shown to be robust to noisy data. Minimizing the empirical risk with the zero-one loss is shown to be equivalent to minimizing the empirical adversarial (worse-case) risk ( [Hu et al 2018](https://arxiv.org/abs/1611.02041) ). Because the worst-case risk is the upper bound of the classification risk of the clean data distribution, minimizing the worst-case risk can lead to decreased true risk, which makes the zero-one loss especially robust. However, the zero-one loss is non-differentiable and cannot be optimized directly. One solution is to approximate an upper bound of the zero-one loss and to minimize the upper bound loss instead.
The [hinge loss](https://en.wikipedia.org/wiki/Hinge_loss) , L hinge ( u ) = ‚àë i = 1 n max ( 0 , 1 ‚àí u i ) , defines a rough upper bound of the zero-one loss. [Lyu & Tsang (2020)](https://arxiv.org/abs/1905.10045) proposed a curriculum loss ( CL ), which is a tighter upper bound compared to a conventional surrogate loss like the hinge loss, L 01 ( u ) ‚â§ L CL ( u ) ‚â§ L hinge ( u ) .
ùüô L CL ( u ) = min w ‚àà { 0 , 1 } n max ( ‚àë i = 1 n w i ‚Ñì ( u i ) , n ‚àí ‚àë i = 1 n w i + ‚àë i = 1 n 1 [ u i < 0 ] )
where ‚Ñì ( u i ) is a base surrogate loss for the zero-one loss (e.g. hinge loss) and the optimal weighting variable w is to be learned.
Given a label corruption rate œÅ , the noise pruned curriculum loss ( NPCL ) is constructed based on the intuition that an ideal model should correctly classify n ( 1 ‚àí œÅ ) samples with clean labels but misclassify n œÅ corrupted labels. If œÅ is a known prior, we would know how many samples (with largest losses) to be pruned. Assuming ‚Ñì ( u 1 ) ‚â§ ‚ãØ ‚â§ ‚Ñì ( u n ) , then u n ( 1 ‚àí œÅ ) + 1 = ‚ãØ = u n = 0 and the following NPCL is the basic CL for only n ( 1 ‚àí œÅ ) samples:
NPCL ( u ) = min w ‚àà { 0 , 1 } n ( 1 ‚àí œÅ ) max ( ‚àë i = 1 n ( 1 ‚àí œÅ ) w i ‚Ñì ( u i ) , n ( 1 ‚àí œÅ ) ‚àí ‚àë i = 1 n ( 1 ‚àí œÅ ) w i )
When experimenting on CIFAR-10, NPCL is comparable with GCE and performs better when the noise rate increases.
## Label Correction [#](#label-correction)
Since it is known some labels are incorrect, noise-robust training can explicitly take the label correction into consideration.
One approach is to rely on the estimation of a noise transition matrix and use that to correct the forward or backward loss, named F-correction ( [Patrini et al. 2017](https://arxiv.org/abs/1609.03683) ). Let‚Äôs first assume that there are k classes and the noise transition matrix C ‚àà [ 0 , 1 ] k √ó k is observable and the label flipping probability does not depend on the sample input but only the label (i.e. known as random classification noise, RCN). Let y ~ denote a corrupted label. Each entry of C represents the probability of one label flipping to another [1](#fn:1) ,
C i j = p ( y ~ = j | y = i , x ) ‚âà p ( y ~ = j | y = i )
Then we can proceed a forward label correction procedure to incorporate the prior knowledge of noisy transition matrix into the prediction.
L ( p ^ ( y ~ | x ) , y ) = ‚àí log ‚Å° p ^ ( y ~ = i | x ) = ‚àí log ‚Å° ‚àë j = 1 k p ( y ~ = i | y = j ) p ^ ( y = j | x ) = ‚àí log ‚Å° ‚àë j = 1 k C j i p ^ ( y = j | x )
In matrix form, we have L ( p ^ ( y | x ) ) = ‚àí log ‚Å° C ‚ä§ p ^ ( y | x ) . However, such a noise transition matrix is usually unknown . If we have access to a clean dataset, the noise matrix C can be estimated ( [Hendrycks et al. 2018](https://arxiv.org/abs/1802.05300) ) by calculating confusion matrix on the clean data. Let‚Äôs denote a clean trusted dataset as D c and a noisy dataset as D n going forward.
C ^ i j = 1 | A i | ‚àë x ‚àà A i p ^ ( y ~ = j | y = i , x ) ‚âà p ( y ~ = j | y = i )
where A i is a subset of data points from D c with label i .
Let f ( x ) = p ^ ( y ~ | x ; Œ∏ ) and this model should be trained with L ( f ( x ) , y ) on clean data D c and with L ( C ^ ‚ä§ f ( x ) , y ^ ) on noisy data D n .
Algorithm of gold loss correction (GLC), estimating the noise transition matrix with a trusted dataset. (Image source: [Hendrycks et al. 2018](https://arxiv.org/abs/1802.05300) )
If the trusted training dataset D c gets large, we can train a neural network only on clean data and distill its knowledge into the primary model (i.e. the final model to make predictions at test time) using corrected pseudo labels ( [Li et al. 2017](https://arxiv.org/abs/1703.02391) ). The primary model is trained on the entire dataset, D = D c ‚à™ D n . Optionally the ‚Äúside‚Äù information of label relations in the knowledge graph, if available, can be incorporated into distillation to help the robustness of the predictions of the network that is trained on limited data.
The label correction distillation works as following:
First train an auxiliary model f c from the small clean dataset D c to provide a soft label for each sample x i , s i = Œ¥ ( f c ( x i ) / T ) is the sigmoid activation with temperature T . Because the clean dataset is not large, f c is likely to overfit, [Li et al. (2017)](https://arxiv.org/abs/1703.02391) turn to a knowledge graph G that defines the relations in the label space and propagate the prediction among labels accordingly. The new soft label is donated as s ^ i = G ( s i ) . The primary model f is trained with predictions from f c to imitate,
L ( y i , f ( x i ) ) = CE ( Œª y i + ( 1 ‚àí Œª ) s ^ i ‚èü pseudo label , f ( x i ) )
## Sample Reweighting and Selection [#](#sample-reweighting-and-selection)
Some samples may be more likely to have inaccurate labels than others. Such estimation gives us intuition on which samples should be weighted less or more in the loss function. However, considering two types of biases in training data, class imbalance and noisy labels, there is actually a contradictory preference ‚Äî We would prefer samples with larger loss to balance the label distribution but those with smaller loss for mitigating the potential noise. Some work ( [Ren et al. 2018](https://arxiv.org/abs/1803.09050) ) thus argue that in order to learn general forms of training data biases, it is necessary to have a small unbiased validation to guide training. The sample reweighting methods presented in this section all assume access to a small trusted set of clean data.
Considering a binary classification task with random classification noise, y , y ^ ‚àà { ‚àí 1 , + 1 } , the label flipping probabilities, œÅ ‚àí 1 , œÅ + 1 ‚àà [ 0 , 0.5 ) , are defined as:
œÅ ‚àí 1 = P ( y ~ = + 1 | y = ‚àí 1 ) œÅ + 1 = P ( y ~ = ‚àí 1 | y = + 1 )
[Liu & Tao (2015)](https://arxiv.org/abs/1411.7718) applies importance reweighting to adjust the weighted distribution of observed y ^ to match the distribution of unobservable y . Let D be the true data distribution and D œÅ be the corrupted version.
L ‚Ñì , D ( f ) = E ( x , y ) ‚àº D [ ‚Ñì ( f ( x ) , y ) ] = E ( x , y ~ ) ‚àº D œÅ [ P D ( x , y = y ~ ) P D œÅ ( x , y ~ ) ‚Ñì ( f ( x ) , y ~ ) ] = E ( x , y ~ ) ‚àº D œÅ [ P D ( y = y ~ | x ) P D œÅ ( y ~ | x ) ‚Ñì ( f ( x ) , y ~ ) ] ; because P D ( x ) = P D œÅ ( x ) = E ( x , y ~ ) ‚àº D œÅ [ w ( x , y ^ ) ‚Ñì ( f ( x ) , y ~ ) ] = L w ‚Ñì , D ( f )
Because,
P D œÅ ( y ~ | x ) = P D ( y = y ~ | x ) P D œÅ ( y ~ | y = y ~ ) + P D ( y = ‚àí y ~ | x ) P D œÅ ( y ~ | y = ‚àí y ~ ) = P D ( y = y ~ | x ) ( 1 ‚àí P D œÅ ( ‚àí y ~ | y = y ~ ) ) + ( 1 ‚àí P D ( y = y ~ | x ) ) P D œÅ ( y ~ | y = ‚àí y ~ ) = P D ( y = y ~ | x ) ( 1 ‚àí œÅ y ~ ) + ( 1 ‚àí P D ( y = y ~ | x ) ) œÅ ‚àí y ~ = P D ( y = y ~ | x ) ( 1 ‚àí œÅ y ~ ‚àí œÅ ‚àí y ~ ) + œÅ ‚àí y ~
Thus the weight assigned to a noisy sample is,
w ( x , y ~ ) = P D ( y = y ~ | x ) P D œÅ ( y ~ | x ) = P D œÅ ( y ~ | x ) ‚àí œÅ ‚àí y ~ ( 1 ‚àí œÅ 0 ‚àí œÅ 1 ) P D œÅ ( y ~ | x )
where P D œÅ ( y ~ | x ) can be estimated using a simple logistic regression, but estimating the note rates is more challenging. Naive cross-validation can work out but is costly as the quality depends on the amount of trusted labels available. The paper approximates the upper bounds for noise rates first, œÅ y ~ ‚â§ P D œÅ ( ‚àí y ~ | x ) and then use a mild assumption to efficiently estimate them, œÅ ^ y ~ = min x ‚àà x 1 , ‚Ä¶ , x n P ^ D œÅ ( ‚àí y ~ | x ) . In their experiments, the advantage of importance reweighting only varies across datasets and is more beneficial when the noise rates are high in general.
Sample reweighting schemes can be learned by a separate network. Learning to reweight ( L2R ; [Ren et al. 2018](https://arxiv.org/abs/1803.09050) ) is a meta-learning approach to directly optimize the weights in pursuit of best validation performance on a known set of clean data. Each example gets assigned with the weight based on its gradient direction. The weighted loss to minimize Œ∏ ‚àó ( w ) involves a set of training weights { w i } i = 1 n as unknown hyperparameters. These sample training weights w i are learned to minimize the loss on this unbiased validate set, D c = { x j valid } j = 1 m .
Œ∏ ‚àó ( w ) = arg ‚Å° min Œ∏ ‚àë i = 1 n w i f ( x i ; Œ∏ ) where optimal w ‚àó = arg ‚Å° min w , w ‚â• 0 1 m ‚àë j = 1 m f ( x j valid ; Œ∏ ‚àó ( w ) )
The learning process involves two nested loops of optimization, so pretty expensive, 3x training time.
Illustration of updates implemented by second order [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) . (Image source: [Ren et al. 2018](https://arxiv.org/abs/1803.09050) )
They ran experiments on (1) two-class MNIST to test the robustness of L2R when the class distribution is imbalanced and (2) CIFAR-10 with noisy labels. L2R is shown to be better than other baseline methods at the time on both tasks.
Left: Imbalanced classes on MNIST (class 4 and 9); Right: Effect of the number of clean samples. Task is on CIFAR-10 with 40% of data flipped to label 3. (Image source: [Ren et al. 2018](https://arxiv.org/abs/1803.09050) )
MentorNet ( [Jiang et al. 2018](https://arxiv.org/abs/1712.05055) ) uses teach-student curriculum learning to weight data. It incorporates two different networks, a mentor and a student. The mentor network provides a data-driven curriculum (i.e. sample training weighting scheme) for the student to focus on learning likely correct labels.
Let g œà be the MentorNet parameterized by œà , f Œ∏ be the StudentNet parametrized by Œ∏ and G be a predefined curriculum parameterized by Œª . Given the training data D = { ( x i , y i ) } i = 1 n for a k -class classification task, the MentorNet needs to predict a time-varying latent weight variable w ‚àà [ 0 , 1 ] n √ó k to guide the learning of StudentNet, taking an intermediate feature processed by StudentNet f , z i = œï f Œ∏ ( x i , y i ) :
g œà ‚àó ( z i ) = arg ‚Å° min w i ‚àà [ 0 , 1 ] L ( Œ∏ , w ) , ‚àÄ i ‚àà [ 1 , n ]
StudentNet learns to minimize the following learning objective,
L ( Œ∏ , w ) = 1 n ‚àë i = 1 n w i ‚ä§ ‚Ñì ( y i , f Œ∏ ( x i ) ) + G Œª ( w ) + Œ± | Œ∏ | 2 2 = 1 n ‚àë i = 1 n g œà ( z i ) ‚ä§ ‚Ñì i + G Œª ( w ) + Œ± | Œ∏ | 2 2 ; Let ‚Ñì i = ‚Ñì ( y i , f Œ∏ ( x i ) )
The mentor network g œà is trained with cross entropy on the input ( œï f Œ∏ ( x i , y i ) , w i ‚àó ) , where v i ‚àó = 1 if y i is known to be a correct label, otherwise 0. The architecture of MentorNet does not have to be very complicated. In the paper, they adopted a LSTM layer to capture the prediction variance in time.
Model architecture of MentorNet and StudentNet which are trained simultaneously, where MentorNet predicts the sample weights for StudentNet to train on. (Image source: [Jiang et al. 2018](https://arxiv.org/abs/1712.05055) )
Different from MentorNet where one network explicitly learns weighting scheme and curriculum for the other network, Co-teaching ( [Han et al. 2018](https://arxiv.org/abs/1804.06872) ) trains two neural networks, f 1 and f 2 , simultaneously and lets them teach each other by feeding data to each other selectively. Co-teaching consists of three steps:
First, each network feeds forward the current mini-batch and selects samples with potentially clean labels; Then two networks exchange information on which samples in the batch should be used for training. Small-loss instances are selected as they are more likely to be associated with correct labels. The percentage of the batch to select is determined by a time-dependent function R ( T ) . The value of R ( T ) decreases in time because the network is more likely to overfit and memorize noisy labels as training progresses and thus we use a smaller sampling percentage to keep the selected data quality high. Finally, each network runs back-propagation updates with the data selected by its peer.
According to their experiments, co-teaching performs better than [F-correction](#fcorrection) where the noise rates are high or the corruption transition matrix is not symmetric.
Algorithm of co-teaching in which two networks are trained separately in parallel and each selects samples for the other to train on. (Image source: [Han et al. 2018](https://arxiv.org/abs/1804.06872) )
# Citation [#](#citation)
Cited as:
Weng, Lilian. (Apr 2022). Learning with not enough data part 3: data generation. Lil‚ÄôLog. https://lilianweng.github.io/posts/2022-04-15-data-gen/.
Or
@article{weng2022datagen,
title = "Learning with not Enough Data Part 3: Data Generation" ,
author = "Weng, Lilian" ,
journal = "Lil'Log" ,
year = "2022" ,
month = "Apr" ,
url = "https://lilianweng.github.io/posts/2022-04-15-data-gen/" } copy
# Reference [#](#reference)
[1] Zhang et al. [‚ÄúAdversarial AutoAgument‚Äù](https://arxiv.org/abs/1912.11188) ICLR 2020.
[2] Kumar et al. [‚ÄúData Augmentation using Pre-trained Transformer Models.‚Äù](https://arxiv.org/abs/2003.02245) AACL 2020 Workshop.
[3] Anaby-Tavor et al. [‚ÄúNot enough data? Deep learning to rescue!‚Äù](https://arxiv.org/abs/1911.03118) AAAI 2020.
[4] Wang et al. [‚ÄúWant To Reduce Labeling Cost? GPT-3 Can Help.‚Äù](https://arxiv.org/abs/2108.13487) EMNLP 2021.
[5] Wang et al. [‚ÄúTowards Zero-Label Language Learning.‚Äù](https://arxiv.org/abs/2109.09193) arXiv preprint arXiv:2109.09193 (2021).
[6] Schick & Schutze. [Generating Datasets with Pretrained Language Models."](https://arxiv.org/abs/2104.07540) EMNLP 2021.
[7] Han et al. [‚ÄúUnsupervised Neural Machine Translation with Generative Language Models Only.‚Äù](https://arxiv.org/abs/2110.05448) arXiv preprint arXiv:2110.05448 (2021).
[8] Guo et al. [‚ÄúAugmenting data with mixup for sentence classification: An empirical study.‚Äù](https://arxiv.org/abs/1905.08941) arXiv preprint arXiv:1905.08941 (2019).
[9] Ekin D. Cubuk et al. [‚ÄúAutoAugment: Learning augmentation policies from data.‚Äù](https://arxiv.org/abs/1805.09501) arXiv preprint arXiv:1805.09501 (2018).
[10] Daniel Ho et al. [‚ÄúPopulation Based Augmentation: Efficient Learning of Augmentation Policy Schedules.‚Äù](https://arxiv.org/abs/1905.05393) ICML 2019.
[11] Cubuk & Zoph et al. [‚ÄúRandAugment: Practical automated data augmentation with a reduced search space.‚Äù](https://arxiv.org/abs/1909.13719) arXiv preprint arXiv:1909.13719 (2019).
[12] Zhang et al. [‚Äúmixup: Beyond Empirical Risk Minimization.‚Äù](https://arxiv.org/abs/1710.09412) ICLR 2017.
[13] Yun et al. [‚ÄúCutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features.‚Äù](https://arxiv.org/abs/1905.04899) ICCV 2019.
[14] Kalantidis et al. [‚ÄúMixing of Contrastive Hard Negatives‚Äù](https://arxiv.org/abs/2010.01028) NeuriPS 2020.
[15] Wei & Zou. [‚ÄúEDA: Easy data augmentation techniques for boosting performance on text classification tasks.‚Äù](https://arxiv.org/abs/1901.11196) EMNLP-IJCNLP 2019.
[16] Kobayashi. [‚ÄúContextual Augmentation: Data Augmentation by Words with Paradigmatic Relations.‚Äù](https://arxiv.org/abs/1805.06201) NAACL 2018
[17] Fang et al. [‚ÄúCERT: Contrastive self-supervised learning for language understanding.‚Äù](https://arxiv.org/abs/2005.12766) arXiv preprint arXiv:2005.12766 (2020).
[18] Gao et al. [‚ÄúSimCSE: Simple Contrastive Learning of Sentence Embeddings.‚Äù](https://arxiv.org/abs/2104.08821) arXiv preprint arXiv:2104.08821 (2020). [ [code](https://github.com/princeton-nlp/SimCSE) ]
[19] Shen et al. [‚ÄúA Simple but Tough-to-Beat Data Augmentation Approach for Natural Language Understanding and Generation.‚Äù](https://arxiv.org/abs/2009.13818) arXiv preprint arXiv:2009.13818 (2020) [ [code](https://github.com/dinghanshen/cutoff) ]
[20] Wang & van den Oord. [‚ÄúMulti-Format Contrastive Learning of Audio Representations.‚Äù](https://arxiv.org/abs/2103.06508) NeuriPS Workshop 2020.
[21] Wu et al. [‚ÄúConditional BERT Contextual Augmentation‚Äù](https://arxiv.org/abs/1812.06705) arXiv preprint arXiv:1812.06705 (2018).
[22 Zhu et al. [‚ÄúFreeLB: Enhanced Adversarial Training for Natural Language Understanding.‚Äù](https://arxiv.org/abs/1909.11764) ICLR 2020.
[23] Affinity and Diversity: Quantifying Mechanisms of Data Augmentation
Gontijo-Lopes et al. 2020 ( [https://arxiv.org/abs/2002.08973](https://arxiv.org/abs/2002.08973) )
[24] Song et al. [‚ÄúLearning from Noisy Labels with Deep Neural Networks: A Survey.‚Äù](https://arxiv.org/abs/2007.08199) TNNLS 2020.
[25] Zhang & Sabuncu. [‚ÄúGeneralized cross entropy loss for training deep neural networks with noisy labels.‚Äù](https://arxiv.org/abs/1805.07836) NeuriPS 2018.
[26] Goldberger & Ben-Reuven. [‚ÄúTraining deep neural-networks using a noise adaptation layer.‚Äù](https://openreview.net/forum?id=H12GRgcxg) ICLR 2017.
[27] Sukhbaatar et al. [‚ÄúTraining convolutional networks with noisy labels.‚Äù](https://arxiv.org/abs/1406.2080) ICLR Workshop 2015.
[28] Patrini et al. [‚ÄúMaking Deep Neural Networks Robust to Label Noise: a Loss Correction Approach‚Äù](https://arxiv.org/abs/1609.03683) CVPR 2017.
[29] Hendrycks et al. [‚ÄúUsing trusted data to train deep networks on labels corrupted by severe noise.‚Äù](https://arxiv.org/abs/1802.05300) NeuriPS 2018.
[30] Zhang & Sabuncu. [‚ÄúGeneralized cross entropy loss for training deep neural networks with noisy labels.‚Äù](https://arxiv.org/abs/1805.07836) NeuriPS 2018.
[31] Lyu & Tsang. [‚ÄúCurriculum loss: Robust learning and generalization against label corruption.‚Äù](https://arxiv.org/abs/1905.10045) ICLR 2020.
[32] Han et al. [‚ÄúCo-teaching: Robust training of deep neural networks with extremely noisy labels.‚Äù](https://arxiv.org/abs/1804.06872) NeuriPS 2018. ( [code](https://github.com/bhanML/Co-teaching) )
[33] Ren et al. [‚ÄúLearning to reweight examples for robust deep learning.‚Äù](https://arxiv.org/abs/1803.09050) ICML 2018.
[34] Jiang et al. [‚ÄúMentorNet: Learning data-driven curriculum for very deep neural networks on corrupted labels.‚Äù](https://arxiv.org/abs/1712.05055) ICML 2018.
[35] Li et al. [‚ÄúLearning from noisy labels with distillation.‚Äù](https://arxiv.org/abs/1703.02391) ICCV 2017.
[36] Liu & Tao. [‚ÄúClassification with noisy labels by importance reweighting.‚Äù](https://arxiv.org/abs/1411.7718) TPAMI 2015.
[37] Ghosh, et al. [‚ÄúRobust loss functions under label noise for deep neural networks.‚Äù](https://arxiv.org/abs/1712.09482) AAAI 2017.
[38] Hu et al. [‚ÄúDoes Distributionally Robust Supervised Learning Give Robust Classifiers? ‚Äú](https://arxiv.org/abs/1611.02041) ICML 2018.
y = i is not a technically correct way to annotate a label being a certain value, since we usually use one-hot encoding (i.e. y = e i ). We use this form for simplicity. [‚Ü©Ô∏é](#fnref:1)
[Data](https://lilianweng.github.io/tags/data/) [Data-Generation](https://lilianweng.github.io/tags/data-generation/) [Data-Augmentation](https://lilianweng.github.io/tags/data-augmentation/) [Data-Quality](https://lilianweng.github.io/tags/data-quality/) [¬´ Generalized Visual Language Models](https://lilianweng.github.io/posts/2022-06-09-vlm/) [¬ª Learning with not Enough Data Part 2: Active Learning](https://lilianweng.github.io/posts/2022-02-20-active-learning/) ¬© 2025 [Lil'Log](https://lilianweng.github.io/) Powered by [Hugo](https://gohugo.io/) & [PaperMod](https://git.io/hugopapermod)