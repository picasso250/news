[Hugging Face](/) [Models](/models) [Datasets](/datasets) [Spaces](/spaces) Community [Docs](/docs) [Enterprise](/enterprise) [Pricing](/pricing) [Log In](/login) [Sign Up](/join) [Back to Articles](/blog)
# HTML_TAG_START GGML and llama.cpp join HF to ensure the long-term progress of Local AI HTML_TAG_END
Published February 20, 2026 [Update on GitHub](https://github.com/huggingface/blog/blob/main/ggml-joins-hf.md) [Upvote 471](/login?next=%2Fblog%2Fggml-joins-hf) +465 [Georgi Gerganov ggerganov Follow](/ggerganov) [Xuan-Son Nguyen ngxson Follow](/ngxson) [Aleksander Grygier allozaur Follow](/allozaur) [Lysandre lysandre Follow](/lysandre) [Victor Mustar victor Follow](/victor) [Julien Chaumond julien-c Follow](/julien-c) HTML_TAG_START
[HTML_TAG_START What will change for llama.cpp, the open source project and the community? HTML_TAG_END](#what-will-change-for-llamacpp-the-open-source-project-and-the-community) [HTML_TAG_START Technical focus HTML_TAG_END](#technical-focus) [HTML_TAG_START Our long term vision HTML_TAG_END](#our-long-term-vision) We are super happy to announce that GGML, creators of Llama.cpp, are joining HF in order to keep future AI open. üî•
[Georgi Gerganov](https://huggingface.co/ggerganov) and team are joining HF with the goal of scaling and supporting the community behind ggml and llama.cpp as Local AI continues to make exponential progress in the coming years.
We've been working with Georgi and team for quite some time (we even have awesome core contributors to llama.cpp like [Son](https://huggingface.co/ngxson) and [Alek](https://huggingface.co/allozaur) in the team already) so this has been a very natural process.
llama.cpp is the fundamental building block for local inference, and transformers is the fundamental building block for model definition, so this is basically a match made in heaven. ‚ù§Ô∏è
### What will change for llama.cpp, the open source project and the community?
Not much ‚Äì Georgi and team still dedicate 100% of their time maintaining llama.cpp and have full autonomy and leadership on the technical directions and the community.
HF is providing the project with long-term sustainable resources, improving the chances of the project to grow and thrive. The project will continue to be 100% open-source and community driven as it is now.
### Technical focus
llama.cpp is the fundamental building block for local inference, and transformers is the fundamental building block for definition of models and architectures, so we‚Äôll work on making sure it‚Äôs as seamless as possible in the future (almost ‚Äúsingle-click‚Äù) to ship new models in llama.cpp from the transformers library ‚Äòsource of truth‚Äô for model definitions.
Additionally, we will improve packaging and user experience of ggml-based software. As we enter the phase in which local inference becomes a meaningful and competitive alternative to cloud inference, it is crucial to improve and simplify the way in which casual users deploy and access local models. We will work towards making llama.cpp ubiquitous and readily available everywhere.
### Our long term vision
Our shared goal is to provide the community with the building blocks to make open-source superintelligence accessible to the world over the coming years.
We will achieve this together with the growing Local AI community, as we continue to build the ultimate inference stack that runs as efficiently as possible on our devices.
HTML_TAG_END
More Articles from our Blog
[llm fine-tuning open-source
## Codex is Open Sourcing AI models
77 December 11, 2025 burtenshaw, et. al.](/blog/hf-skills-training-codex) [llm fine-tuning open-source Hot
## We Got Claude to Fine-Tune an Open Source LLM
605 December 4, 2025 burtenshaw, et. al.](/blog/hf-skills-training)
### Community
[Bright8192](/Bright8192) [9 days ago](#6998809ba1d2cd80255e509c)
Big congrats to GGML and Hugging Face! Great news for the Local AI community. Excited to see llama.cpp grow stronger and make local AI easier for everyone!
See translation 4 replies ¬∑ üî• 16 16 + [Adamqubit](/Adamqubit) [8 days ago](#699b07fd4ce2be5ae430859c) This comment has been hidden (marked as Off-Topic) Expand 3 replies [Room64](/Room64) [9 days ago](#69988a512ccc26f7b657ae8a)
LLama.cpp is the best AI project by far, super reactive to bug solve, very competent team, love you guys, you desserve it
See translation ‚ù§Ô∏è 22 22 üî• 3 3 üëç 2 2 üöÄ 1 1 üëÄ 1 1 ü§ó 1 1 üòé 1 1 ‚ûï 1 1 üß† 1 1 ü§ù 1 1 üòî 1 1 ü§Ø 1 1 + Reply [Xenova](/Xenova) [9 days ago](#69988f8ff798fc9b8e52b364)
Our shared goal is to provide the community with the building blocks to make open-source superintelligence accessible to the world over the coming years.
See translation üî• 45 45 + Reply [Trilogix1](/Trilogix1) [9 days ago](#6998b4d1eb5fa1d68ccae115)
Hugging Face smart moves never ending. Are you guys using AI for advice? I wonder which of 2 million AI models you are using üòÑ
See translation üëÄ 3 3 + Reply [joshnur](/joshnur) [9 days ago](#6998ba892f94705daa714b96)
Great news.
Serving with llama.cpp using HF-hosted models, including unsloth's on AMD Strix Halo and OpenCode here.
See translation ‚ù§Ô∏è 2 2 + Reply [raphaelamorim](/raphaelamorim) [9 days ago](#6998c81c8f9e9699b59ca38f) ‚Ä¢ [edited 9 days ago](#6998c81c8f9e9699b59ca38f)
Congrats to both teams. Well deserved. Wonderful news for wonderful teams and community.
See translation ü§ó 2 2 + Reply [iyanello](/iyanello) [9 days ago](#6998cb9a8c60170028a36f55)
Congratulations to Georgi Gerganov and team! So happy for you guys, this is huge success!
See translation ‚ù§Ô∏è 1 1 + Reply [Tugay31](/Tugay31) [9 days ago](#6998e4b24ce2be5ae4308558)
Great news. congrats to GGML and HF. . always LocalAI.
See translation ‚ù§Ô∏è 1 1 + Reply [arkavo-paul](/arkavo-paul) [9 days ago](#6998f2975dc85662c9645054)
This is a match made in heaven for the local AI ecosystem. Transformers as the model definition layer plus llama.cpp as the local inference layer, backed by HF's long-term resources, gives the entire community a stable foundation to build on for years to come.
The focus on packaging and user experience is especially important. Making local inference accessible beyond developers is how we get to an AI future that's open, private, and user-owned ‚Äî not locked behind API calls.
Congratulations to Georgi and team. Open-source superintelligence that runs on your own hardware isn't just a technical goal, it's a trust model.
See translation üî• 5 5 + Reply [simeks18](/simeks18) [9 days ago](#69992392d3b97b3e64d78d7b)
Congratulations! I love Llama.cpp and I love running my models locally. This is absolutely the future of transparency and I love the push for the open, private, user-owned software world! Thank you for all that you are doing!
See translation üî• 4 4 + Reply [tuaris](/tuaris) [9 days ago](#69993967f798fc9b8e52b39b) ‚Ä¢ [edited 9 days ago](#69993967f798fc9b8e52b39b)
So basically HF, "acquires" an open source project. hmm. I've seen this before and it never ends well (see Trixbox, PCBSD, FreeNAS, etc..).
I sure hope history doesn't repeat itself (yet it always does).
See translation üëÄ 4 4 + Reply [jimenezcarrero](/jimenezcarrero) [9 days ago](#69995c77055c4637da886fc4)
It‚Äôs great news for the future of edge AI!
See translation üî• 1 1 + Reply [woctordho](/woctordho) [9 days ago](#699977f3e6446c2a6084e6b9)
Please also acquire ik_llama
See translation Reply [Tonic](/Tonic) [9 days ago](#699978f07a80466999001ed4)
gglm's gguf format now the prefered default for executorch (on device) inference üöÄü¶ô
See translation ü§ó 1 1 + Reply [CyberMas](/CyberMas) [8 days ago](#6999d6cb5f102e9345d24f67) This comment has been hidden (marked as Off-Topic) [pulak007](/pulak007) [8 days ago](#6999e83eeb5fa1d68ccae141)
niceee.
Reply [sverinn](/sverinn) [8 days ago](#699a186b5597d4a3f304961b)
finally, something good about living in modern world, you guys are awesome!
See translation Reply [SqueezingFace](/SqueezingFace) [8 days ago](#699a255a5597d4a3f3049634)
"...it is crucial to improve and simplify the way in which casual users deploy and access local models. We will work towards making llama.cpp ubiquitous and readily available everywhere." (It... already was?)
Before you upvote. Raise your hand if you realize that hf.co is a business with the necessary end goal of making money . This isn't a bad thing; however, this blog post is so devoid of substance and so full of hypebole that one can't help but wonder.
See translation 1 reply ¬∑ [pszemraj](/pszemraj) [6 days ago](#699cb53850ff2b6f7ec90cc4) ‚Ä¢ [edited 6 days ago](#699cb53850ff2b6f7ec90cc4)
I cant wait till hf adds a quota/limitation on amount of models you can quantize with future versions of llama.cpp requiring hf login /token to quantize a model (trust me bro its just basic telemetry) üòª
of course, when the noose tightens further it won't be officially discussed/acknowledged ( why would we?? we have so much to share with sell to the #community like this robot! look its so cute )
See translation [clover-supply](/clover-supply) [8 days ago](#699a3a25f798fc9b8e52b3c0)
Maybe lcpp will now natively support image models quanting? yay
See translation 2 replies ¬∑ [Henk717](/Henk717) [7 days ago](#699b8222d3b97b3e64d78dd2)
Check out stable-diffusion.cpp for this, or KoboldCpp if you want a fork that has both llamacpp and stablediffusioncpp integrated.
See translation Expand 1 reply [salihfurkaan](/salihfurkaan) [8 days ago](#699abcf42f94705daa714bde)
This is awesome news! Making llama.cpp and the GGML ecosystem more sustainable and widely supported will help local AI become more accessible and easier to use for everyone for sure.
See translation üî• 2 2 + Reply [rombodawg](/rombodawg) [5 days ago](#699de9e08bd7bfabb232fa1c)
Does this mean that we will have GGUF quants of models as they release, or at least support for gguf out of the box for new models in the future?
See translation Reply [AkujinLiffy](/AkujinLiffy) [5 days ago](#699e1f50d08eda32be23b6d3)
reasoning: min_steps: 2 # Minimum reasoning steps before code require_action_field: true # Each step must have thought + action = freedom confidence_calibration: true # Post-process confidence scores ciritcally high!
Congrads!
See translation Reply [datayoda](/datayoda) [4 days ago](#699fe8ac1355fbca8f995a99)
Get Pi as the agent harness next
See translation 1 reply ¬∑ üëç 1 1 + [julien-c](/julien-c) Article author [3 days ago](#69a05cb5456509db2cf66cf5)
cc: [@ victor](/victor)
üëç 1 1 + [Serveurperso](/Serveurperso) [3 days ago](#69a073e8bf572ab76a2ad97f)
A great milestone for Local AI! For those already living daily in the ggml and llama.cpp ecosystem, this is a strong signal for what‚Äôs ahead. The alignment with Transformers brings clear strategic coherence. A solid move. Looking forward to what comes next.
See translation 1 reply ¬∑ ‚ù§Ô∏è 1 1 + [julien-c](/julien-c) Article author [3 days ago](#69a07407fb68ce15c2f9c498)
üî•
[scthornton](/scthornton) [3 days ago](#69a0b4cb847c1a10dac3a825)
Congratulations to all involved! These are great additions!
See translation Reply Edit Preview Upload images, audio, and videos by dragging in the text input, pasting, or clicking here . Tap or paste here to upload images Comment
¬∑ [Sign up](/join?next=%2Fblog%2Fggml-joins-hf) or [log in](/login?next=%2Fblog%2Fggml-joins-hf) to comment
[Upvote 471](/login?next=%2Fblog%2Fggml-joins-hf) +459 System theme Company [TOS](/terms-of-service) [Privacy](/privacy) [About](/huggingface) [Careers](https://apply.workable.com/huggingface/) Website [Models](/models) [Datasets](/datasets) [Spaces](/spaces) [Pricing](/pricing) [Docs](/docs) Stripe