[Hugging Face](/) [Models](/models) [Datasets](/datasets) [Spaces](/spaces) Community [Docs](/docs) [Enterprise](/enterprise) [Pricing](/pricing) [Log In](/login) [Sign Up](/join) [Back to Articles](/blog)
# HTML_TAG_START One Year Since the ‚ÄúDeepSeek Moment‚Äù HTML_TAG_END
[Team Article](/blog) Published January 20, 2026 [Upvote 60](/login?next=%2Fblog%2Fhuggingface%2Fone-year-since-the-deepseek-moment) +54 [Adina Yakefu AdinaY Follow](/AdinaY) [huggingface](/huggingface) [Irene Solaiman irenesolaiman Follow](/irenesolaiman) [huggingface](/huggingface) HTML_TAG_START
[HTML_TAG_START The Seeds of China‚Äôs Organic Open Source AI Ecosystem HTML_TAG_END](#the-seeds-of-chinas-organic-open-source-ai-ecosystem) [HTML_TAG_START DeepSeek R1: A Turning Point HTML_TAG_END](#deepseek-r1-a-turning-point) [HTML_TAG_START From DeepSeek to AI+: Strategic Realignmentt HTML_TAG_END](#from-deepseek-to-ai-strategic-realignmentt) [HTML_TAG_START Global Reception and Response HTML_TAG_END](#global-reception-and-response) This is the first blog in a series that will examine China‚Äôs open source community‚Äôs historical advancements in the past year and its reverberations in shaping the entire ecosystem. Much of 2025‚Äôs progress can be traced back to January‚Äôs ‚ÄúDeepSeek Moment‚Äù, when Hangzhou-based AI company DeepSeek released their R-1 model.
The first blog addresses strategic changes and the explosion of new open models and open source players. The second covers architectural and hardware choices largely by Chinese companies made in the wake of a growing open ecosystem, available [here](https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment-blog-2) . The third analyzes prominent organizations‚Äô trajectories and the future of the global open source ecosystem, available [here](https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment-blog-3) .
For AI researchers and developers contributing to and relying on the open source ecosystem and for policymakers understanding the rapidly changing environment, there has never been a better time to build and release open models and artifacts, as proven by the past year‚Äôs immense growth catalyzed by DeepSeek. Notably, geopolitics has driven adoption; while models developed in China have been dominating across metrics throughout 2025 and new players leapfrogging each other, Western AI communities are seeking commercially deployable alternatives.
## The Seeds of China‚Äôs Organic Open Source AI Ecosystem
Before R1, China‚Äôs AI industry was still largely centered on closed models. Open models had existed for years, but they were mostly confined to research communities or used only in niche scenarios such as privacy-sensitive applications. For most companies, they were not the default choice. Compute resources were tight, and whether to ‚Äúopen or close‚Äù was a subject of debate.
[DeepSeek‚Äôs R1 model](https://huggingface.co/deepseek-ai/DeepSeek-R1) lowered the barrier to advanced AI capabilities and offered a clear pattern to follow, unlocking a second layer. Moreover, the release gave Chinese AI development something extremely valuable: time. It showed that even with limited resources, rapid progress was still possible through open source and fast iteration. This approach aligned naturally with the goals set out in China‚Äôs 2017 ‚ÄúAI+‚Äù strategy: combining AI with industry as early as possible, while continuing to build up compute capacity over the long term.
One year after the release of R1, what we see emerging is not only a collection of new models, but also a growing organic open source AI ecosystem.
## DeepSeek R1: A Turning Point
For the first time, an open model from China entered the global mainstream rankings and, over the following year, was repeatedly used as a reference point when new models were released. DeepSeek‚Äôs R1 quickly became the most liked model on Hugging Face of all time and the top liked models are no longer majority U.S.-developed.
But R1‚Äôs real significance was not whether it was the strongest model at the time, its importance lay in how it lowered three barriers.
The first was the technical barrier. By openly [sharing](https://www.nature.com/articles/s41586-025-09422-z) its reasoning paths and post-training methods, R1 turned advanced reasoning, previously locked behind closed APIs, into an engineering asset that could be downloaded, distilled, and fine-tuned. Many teams no longer needed to train massive models from scratch to gain strong reasoning capabilities. Reasoning started to behave like a reusable module, applied again and again across different systems. This also pushed the industry to rethink the relationship between model capability and compute cost, a shift that was especially meaningful in a compute-constrained environment like China.
The second was the adoption barrier. R1 was released under the MIT license, making its use, modification, and redistribution straightforward. Companies that had relied on closed models began bringing R1 directly into production. Distillation, secondary training, and domain-specific adaptation became routine engineering work rather than special projects. As distribution constraints fell away, the model quickly spread into cloud platforms and toolchains, and community discussions shifted from ‚Äúwhich model scores higher‚Äù to ‚Äúhow do we deploy it, reduce cost, and integrate it into real systems.‚Äù Over time, R1 moved beyond being a research artifact and became a reusable engineering foundation.
The third change was psychological. When the question shifted from ‚Äúcan we do this?‚Äù to ‚Äúhow do we do this well?‚Äù, decision-making across many companies changed. For the Chinese AI community, this was also a rare moment of sustained global attention, one that mattered deeply to an ecosystem long seen mainly as a follower.
Together, the lowering of these three barriers meant that the ecosystem began to gain the ability to replicate itself.
## From DeepSeek to AI+: Strategic Realignmentt
Once open source moved into the mainstream, a natural question followed: how would Chinese companies' strategies change?Over the past year, the answer became clear: competition began to shift from model-to-model comparisons toward system level capabilities.
Compared with 2024, the period after the release of R1 saw China‚Äôs AI landscape settle into a new pattern. Large technology companies took the lead, startups followed quickly, and companies from vertical industries increasingly entered the field. While their paths differed, a shared understanding gradually emerged, especially among leading players: open source was no longer a short-term tactic, but part of long-term competitive strategy.
The number of competitive Chinese organizations releasing state of the art models and repositories skyrocketed. Reflected in Hugging Face Repository Growth of Chinese Companies , the number of open releases from existing giants substantially increased, with [Baidu](https://huggingface.co/baidu) going from zero releases on Hugging Face in 2024 to over 100 in 2025, and others such as [ByteDance](https://huggingface.co/ByteDance) and [Tencent](https://huggingface.co/tencent) increasing releases by eight to nine times. An influx of newly open organizations released highly performant models, with [Moonshot](https://huggingface.co/moonshotai) ‚Äôs open release, [Kimi K2](https://huggingface.co/moonshotai/Kimi-K2-Thinking) , being a ‚Äú [another DeepSeek moment](https://www.interconnects.ai/p/kimi-k2-and-when-deepseek-moments) ‚Äù.
Releases became stronger and frequent, with performant models released on a weekly basis; newly created Chinese models consistently became most liked and downloaded every week, boasting highest popularity among the top most downloaded new models on Hugging Face. The Top Newly Created Models by Week on Hugging Face shows new repositories labeled by organization location or base model organization location for popular derivatives.
As seen in Hugging Face‚Äôs [heatmap data](https://huggingface.co/spaces/zh-ai-community/model-release-heatmap-zh) , between February and July 2025, open releases from Chinese companies became noticeably more active. Baidu and Moonshot moved from primarily closed approaches toward open release. [Zhipu AI](https://huggingface.co/zai-org) ‚Äôs [GLM](https://huggingface.co/collections/zai-org/glm-47) and Alibaba‚Äôs [Qwen](https://huggingface.co/Qwen) went a step further, expanding from simply publishing model weights to building engineering systems and ecosystem interfaces. At this stage, comparing raw model performance alone was no longer enough to win. Competition increasingly centered on ecosystems, application scenarios, and infrastructure_._
This strategy was effectively successful; of newly created models (<1 year), downloads for Chinese models have surpassed any other country including the U.S.
Chinese AI players are not coordinating by agreement, but by constraint. What looks like collaboration is better understood as alignment under shared technical, economic, and regulatory pressures. This does not mean companies formed cooperative alliances. Rather, under similar constraints around compute, cost, and compliance, they began competing along similar technical foundations and engineering paths. When competition takes place on comparable system structures, the ecosystem starts to show the ability to spread and grow on its own. Tech leaders from Zhipu AI (Z.ai), Moonshot AI, Alibaba‚Äôs Qwen, and Tencent coordinating on shared questions is rarely seen in other countries.
## Global Reception and Response
Positive sentiment toward open source adoption and development has increased worldwide and especially in the U.S., with broader recognition of how open source leadership is critical in global competitiveness.
DeepSeek has been [heavily adopted](https://www.microsoft.com/en-us/corporate-responsibility/topics/ai-economy-institute/reports/global-ai-adoption-2025/) in global markets, especially in Southeast Asia and Africa.¬† In these markets, factors such as multilingual support, open-weight availability, and cost considerations have supported enterprise use.
Often Western organizations seek non-Chinese models for commercial deployment. Major releases from U.S. organizations such as [OpenAI‚Äôs gpt-oss](https://huggingface.co/collections/openai/gpt-oss) , [AI2‚Äôs Olmo](https://huggingface.co/collections/allenai/olmo-31) , and [Meta‚Äôs Llama 4](https://huggingface.co/collections/meta-llama/llama-4) received community engagement. Reflection AI [announced](https://techcrunch.com/2025/10/09/reflection-raises-2b-to-be-americas-open-frontier-ai-lab-challenging-deepseek/) its efforts to build frontier American open-weight models. In France, Mistral released their [Mistral Large 3 family](https://huggingface.co/collections/mistralai/mistral-large-3) , continuously developing their open source roots.
At the same time, major releases in the West build on Chinese models; in November 2025, Deep Cogito released [Cogito v2.1](https://huggingface.co/blog/deepcogito/cogito-v2-1) as the leading U.S. open-weight model. The [model](https://huggingface.co/deepcogito/cogito-671b-v2.1) was a fine-tuned version of DeepSeek-V3. Startups and researchers globally using open-weight models are often defaulting to if not relying on models developed in China.
The [American Truly Open Model (ATOM) project](https://www.atomproject.ai/) cites DeepSeek and China‚Äôs model momentum as a motivator for concerted efforts toward leading in open-weight model development. The project emphasizes the need for multiple efforts and its research also highlights OpenAI‚Äôs [gpt-oss heavy early adoption](https://atomproject.ai/relative-adoption-metric) .
The world is still responding, with a new open source fervor. 2026 is shaping up for major releases , especially from China and the U.S. Of high relevance are the architectural trends, hardware choices, and organizational directions, which will be covered next in this series.
All data represented is sourced from [Hugging Face](https://huggingface.co/datasets/cfahlgren1/hub-stats) . For more related data and analyses of 2025 in open source, we encourage you to read the Data Provenance Initiative and Hugging Face‚Äôs [Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem](https://www.dataprovenance.org/economies-of-open-intelligence.pdf) , aiWorld‚Äôs [Open Source AI Year In Review 2025](https://huggingface.co/spaces/aiworld-eu/Open-Source-AI-Year-in-Review-2025) , and InterConnects‚Äôs [8 plots that explain the state of open models](https://www.interconnects.ai/p/8-plots-that-explain-the-state-of) .
HTML_TAG_END
### Community
[vansin](/vansin) [Jan 20](#696fa4dac121ffeb90c69df0)
Chinese Open Source Model Take off !!!!
See translation üöÄ 3 3 üî• 2 2 + Reply [vansin](/vansin) [Jan 20](#696fa76b6a351f031c246a08)
Chinese Translation of this ArticleÔºö [https://huggingface.co/blog/vansin/one-year-since-the-deepseek-moment-cn](https://huggingface.co/blog/vansin/one-year-since-the-deepseek-moment-cn)
See translation ‚ù§Ô∏è 2 2 + Reply [cfahlgren1](/cfahlgren1) [Jan 20](#696fa85e844a32ce0c55929c)
great job [@ irenesolaiman](/irenesolaiman) üëè
See translation ‚ù§Ô∏è 3 3 + Reply [mahimairaja](/mahimairaja) [Jan 21](#6970d618939aabd59e034ae0)
Great read!
‚ù§Ô∏è 2 2 + Reply [NJX-njx](/NJX-njx) [Jan 28](#697a06479d2b27c38d37d546)
As a Chinese AI researcher who has fully experienced the wave of AI technologies and products in 2025, looking back on the series of changes brought about starting from DeepSeek, I have mixed feelings.
The author's analysis is excellent. The open-sourcing of the R1 model weights and related technologies empowered the most usable technologies to both startups and large companies at that time. Such an "unconventional" change pushed China's AI field, whether in research and development or product development, into a stage of rapid development.
Thanks to open source. May AI technology benefit all of humanity.
See translation ‚ù§Ô∏è 5 5 + Reply [RichardBian](/RichardBian) [about 1 month ago](#697cdd216b6fc59bea030673)
This is such a fair and square article highlighting the technology and open trends. The wave of open models from China was a gift to the overall technical community and forever altered the progress of 2025 and beyond. 2026 will be another year of open models. It might also be the year to reveal how "open source" would be truly redefined in the next 5 years.
See translation ‚ù§Ô∏è 2 2 + Reply Edit Preview Upload images, audio, and videos by dragging in the text input, pasting, or clicking here . Tap or paste here to upload images Comment
¬∑ [Sign up](/join?next=%2Fblog%2Fhuggingface%2Fone-year-since-the-deepseek-moment) or [log in](/login?next=%2Fblog%2Fhuggingface%2Fone-year-since-the-deepseek-moment) to comment
[Upvote 60](/login?next=%2Fblog%2Fhuggingface%2Fone-year-since-the-deepseek-moment) +48 System theme Company [TOS](/terms-of-service) [Privacy](/privacy) [About](/huggingface) [Careers](https://apply.workable.com/huggingface/) Website [Models](/models) [Datasets](/datasets) [Spaces](/spaces) [Pricing](/pricing) [Docs](/docs) Stripe